<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Arsh Tangri</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
		<link rel="icon" type="image/png" sizes="16x16" href="images/my_image.jpg">
		<link rel="manifest" href="images/favicon/site.webmanifest">



		<style>
			.work-links a {
			display: inline-block;
			margin-right: 10px; /* Adjust the spacing between links as desired */
			}

			.work-links::after {
				/* content: " | "; */
				margin-left: 5px; /* Adjust the spacing between the "|" symbol and the links */
				color: #999; /* Adjust the color of the "|" symbol */
				text-decoration: none; /* Remove underline from the "|" symbol */
			}

			.work-item {
				text-align: center;
			}

			.work-title {
				text-align: left;
			}

			.image-container a {
				display: inline-block;
				margin-right: 10px; /* Adjust the spacing between images as desired */
			}

			.image-container img {
				width: 200px; /* Set the desired width of the images */
				height: 217px; /* Set the desired height of the images */
			}

			.winner-symbol {
				font-size: 20px; /* Adjust the size of the winner symbol as desired */
			}

			.highlight-line {
				border: none;
				height: 2px;
				background-color: #ff0000; /* Adjust the color of the highlight line as desired */
				margin: 1px 0; /* Adjust the margin around the highlight line as desired */
			}

			.highlight-text {
				color: #ff0000; /* Adjust the color of the highlight text as desired */
				font-weight: bold; /* Add font-weight or other styles as desired */
			}

			.institution-images {
				display: flex;
				justify-content: center;
				align-items: center;
			}

			.institution {
				text-align: center;
				margin: 0 10px;
			}

			.institution img {
				width: 100px; /* Adjust the width of the institution images as desired */
				height: 100px; /* Adjust the height of the institution images as desired */
				object-fit: contain; /* Adjust the object-fit property as desired to control how the image fills its container */
			}

			.caption {
				margin-top: 5px; /* Adjust the spacing between the image and caption as desired */
				font-size: 14px; /* Adjust the font size of the caption as desired */
			}
			.left-align {
				text-align: justify;
			}
			.right-align {
				text-align: right;
			}
			p {
				color: gray
			}
				/* a {
			color: lightgreen;
			} */

			/* .header-content {
				position: absolute;
				right: 5;
			} */


		</style>

	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar"><img src="images/my_image.jpg" alt="" /></a>
					<h1><strong>Arsh </strong><br /></h1>
					<h1><strong>Tangri</strong></h1>
					<br />
					<!-- <div class="header-content"> -->

					<div class="right-align">
					 <a href="#About">About me</a> <br />
					 <a href="#excitingwork">Recent Work</a> <br />
					 <a href="#Contact">Contact</a> <br />
					 <!-- replace with my resume below -->
					 <a href="images/Arsh_Resume-1.pdf" target="_blank">Resume</a> <br />
					 <!-- <a href="">CV</a> <br /> -->
					</div>
				<!-- </div> -->
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="About">
						<header class="major">
							<h2>About Me</h2>
						</header>
						<div class="left-align">
						<p>I am a Master of Science in Robotics student at Northeastern University, passionate about applying machine learning to solve complex robotics problems. 
							This passion has driven my deep exploration into Deep Learning, Computer Vision, Reinforcement Learning, and other facets of robotics.
							I am currently a researcher at the <a href="https://www2.ccs.neu.edu/research/helpinghands/"  target="_blank">Helping Hands Lab</a> under the supervision of <a href="https://www.khoury.northeastern.edu/people/robert-platt/" target="_blank">Prof. Robert Platt</a>. 
							My work focuses primarily on developing general Reinforcement Learning and Imitation Learning algorithms for robotic manipulation tasks 
							, with a focus on leveraging rotation-equivariant CNNs for improved performance and sample-efficiency.
						</p>						
						<p>Previously, I have completed a 6-month internship at <a href="https://amazon.jobs/en/teams/amazon-robotics"  target="_blank">Amazon Robotics</a>, 
							where I worked on training PointNet-based neural-network policies to map scene point clouds to 6-DoF grasp poses, enabling robotic picking of raw, unpacked Amazon items. 
							I was also a MITACS Research Intern at <a href="https://www.etsmtl.ca/"  target="_blank">ETS, Montreal</a>, where I worked under the supervision of 
							<a href="https://profs.etsmtl.ca/sandrews/"  target="_blank">Prof. Sheldon Andrews</a> on developing a novel framework for synthesizing natural user-styled get-up motions on various challenging terrains for simulated
							Physics-Based humanoid characters using Deep Reinforcement Learning
						</p>
						<p>Before joining Northeastern, I graduated with a B.E. in Electronics and Communications Engineering from the Manipal Institute of Technology, Manipal, India (2022).
						As a part of my final-year project, I conducted research in utilizing self-supervised pre-training on unlabelled images for the improving classification performance 
						for the task of Flooded-Region Classification in aerial images.
						</p>
						</div>
					</section>


					<section id="excitingwork">
						<header class="major">
							<h2>Experience & Projects</h2>
						</header>
						<div class="row">


							<article class="col-6 col-12-xsmall work-item">
								<a href="images/block-insertion.gif" class="image fit thumb"><img src="images/block-insertion.gif" alt="" height="310" /></a>
								<h3> Leveraging Symmetries in Pick and Place </h3>
								 <hr class="highlight-line">
								<p class="highlight-text">Accepted to The International Journal of Robotics Research (Volume 43, Issue 4, 2024)</p>
								<hr class="highlight-line">
								<div class="work-links">
									<a href="https://arxiv.org/abs/2308.07948" target="_blank">Paper</a>
									<a href="https://github.com/HaojHuang/Equivariant-Transporter-Net" target="_blank">Code</a>
								</div>
								<div class="left-align">
								<p><br>
									In this project, we focused on improving the efficiency and generalization of robotic pick-and-place tasks by leveraging symmetries in object manipulation. 
									Standard pick-and-place actions exhibit inherent symmetries under translations and rotations of both the object and the target placement.
									To better capture these symmetries, we extended the existing Transporter Net framework, which only accounts for partial symmetries, by integrating equivariant neural models.
									Our new model, Equivariant Transporter Net, is fully equivariant to both pick and place symmetries, allowing it to generalize pick-and-place actions across various poses with minimal human demonstrations. 
									Empirical evaluations demonstrated that our model is significantly more sample-efficient, making it highly effective in imitation learning tasks with limited training data.
								</p>
							</div>
							</article>

							<article class="col-6 col-12-xsmall work-item">
								<a href="images/Block_in_Bowl_Equi_ORL.gif" class="image fit thumb"><img src="images/Block_in_Bowl_Equi_ORL.gif" alt="" height="310" /></a>
								<h3> Equivariant Offline Reinforcement Learning </h3>
								<div class="work-links">
									<a href="https://arxiv.org/abs/2406.13961" target="_blank">Paper</a>
								</div>
								<div class="left-align">
								<p><br>
									This project addresses the challenge of sample efficiency in robotic manipulation tasks, particularly when applying learning-based methods in environments where expert demonstrations are costly and online policy learning through Reinforcement Learning (RL) is impractical. 
									To overcome these limitations, we explored the use of SO(2)-equivariant neural networks for Offline RL, a framework that allows policy learning from pre-collected datasets. 
									By focusing on the rotation symmetry inherent in many manipulation tasks, we developed equivariant versions of two key offline RL algorithms: Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL). 
									Our empirical results demonstrate that these equivariant models significantly outperform their non-equivariant counterparts, particularly in low-data environments, showing how leveraging symmetries can improve performance and sample efficiency in robotic manipulation.
								</p>
							</div>
							</article>


							<article class="col-6 col-12-xsmall work-item">
								<a href="images/flooded_image.jpg" class="image fit thumb"><img src="images/flooded_image.jpg" alt="" height="310" /></a>
								<h3> Comparison of Texture Classifiers with Deep Learning Methods for Flooded Region Identification in UAV Aerial Images </h3>
								<hr class="highlight-line">
								<p class="highlight-text">Accepted to IEEE International Symposium on Geoscience and Remote Sensing (2022)</p>
								<hr class="highlight-line">
								<div class="work-links">
									<a href="https://ieeexplore.ieee.org/document/9884718" target="_blank">Paper</a>
								</div>
								<div class="left-align">
								<p><br>
									In this project, we addressed the challenge of rapidly classifying flood-affected areas in UAV-captured imagery, crucial for effective post-disaster response. 
									Our approach leveraged self-supervised learning, specifically SimCLR, to improve the efficiency of flood scene classification. 
									SimCLR allows for learning rich feature representations from unlabeled data, which is particularly advantageous in disaster scenarios where labeled data may be scarce.
									We compared the performance of SimCLR to traditional texture-based classifiers, such as those utilizing gray-level 
									co-occurrence matrix (GLCM) and local binary patterns (LBP), and supervised deep learning methods like ResNet18.
									Using the FloodNet dataset, containing UAV images from hurricane Harvey, we found that SimCLR achieved an F1 score of 0.87 for detecting flooded areas, 
									outperforming the traditional classifiers while requiring minimal labeled data. The results demonstrate the potential of self-supervised learning for 
									flood classification in UAV imagery, enabling rapid deployment with reduced need for human intervention.
								</p>
							</div>
							</article>


							<article class="col-6 col-12-xsmall work-item">
								<a href="images/Get_Up.gif" class="image fit thumb"><img src="images/Get_Up.gif" alt="" height="310" /></a>
								<h3> Synthesizing Get-Up Motions for Physics-based Characters </h3>
								<hr class="highlight-line">
								<p class="highlight-text">Accepted to the 21st annual Symposium on Computer Animation (SCA 2022) and 
									published in Computer Graphics Forum (CGF). </p>
								<hr class="highlight-line">
								<div class="work-links">
									<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14636" target="_blank">Paper</a>
									<a href="http://profs.etsmtl.ca/sandrews/publication/getup_sca2022/" target="_blank">Webpage</a>
								</div>
								<div class="left-align">
								<p><br>
									In this project, we present a method for synthesizing get-up motions for physics-based humanoid characters, starting from either a supine or prone state. 
									Rather than imitating specific motion clips, our approach generates motions that align with user-defined input curves that represent the style of the get-up motion. 
									By leveraging deep reinforcement learning, we train control policies for a physics-based character. 
									A latent embedding of natural human poses is derived from a motion capture database, and this embedding is conditioned on the input features to influence the style of the motions. 
									Our approach can synthesize a variety of get-up motions by adjusting only a few controllable parameters, enabling flexibility in motion generation. 
									Furthermore, we showcase the ability of our method to replicate reference motions and handle complex environments like rough or inclined terrain, 
									while maintaining the desired style.
							</div>
							</article>




							<article class="col-6 col-12-xsmall work-item">
								<a href="images/contrastive_rl.png" class="image fit thumb"><img src="images/contrastive_rl.png" alt="" height="310" /></a>
								<h3> Pytorch Implementation of Contrastive Learning As Goal-Conditioned RL								</h3>
								<div class="work-links">
									<a href="https://github.com/ArshT/Contrastive-Learning-As-Goal-Conditioned-RL-Pytorch" target="_blank">Code</a>
								</div>
								<div class="left-align">
								<p><br>
									In this project, I wrote a PyTorch-based implementation for a novel Contrastive Reinforcement Learning (RL) method.
									The paper explores how contrastive representation learning can be directly integrated into RL algorithms, avoiding the need for additional representation learning components like auxiliary losses or data augmentation. 
									By applying contrastive learning to action-labeled trajectories, the learned representations correspond to a goal-conditioned value function. 
									My PyTorch implementation reinterprets existing RL methods through this contrastive lens and proposes a simplified approach that achieves high performance. 
									Through my implementation, I demonstrated how this method outperforms non-contrastive techniques across goal-conditioned RL tasks, including image-based and offline RL tasks, without requiring additional data augmentation or auxiliary objectives.
								</p>
							</div>
							</article>

							<article class="col-6 col-12-xsmall work-item">
								<a href="images/ARMBench_Object_Segmentation.jpg" class="image fit thumb"><img src="images/ARMBench_Object_Segmentation.jpg" alt="" height="310" /></a>
								<h3> ARMBENCH Object Segmentation </h3>
								<div class="work-links">
									<a href="https://github.com/ArshT/ARMBENCH_Object_Segmentation" target="_blank">Code</a>
								</div>
								<div class="left-align">
								<p><br>
									In this project, we aimed to accurately segment multiple object instances and the tote containing them using the Segment-Anything Model (SAM) from MetaAI. 
									We employed the DETR Object Detection model to generate bounding boxes, which served as prompts for SAM.
									Additionally, we fine-tuned the SAM decoder for improved performance. 
									The project used the ARMBench Dataset from Amazon Robotics, and the fine-tuned model successfully segmented both the objects and the tote with high accuracy.
								</p>
							</div>
							</article>
						</div>
					</section>


					<section id="Contact">
						<h2>Let's Get In Touch!</h2>
						<p>Reach out to me through mail or using any of the links on the side.
						<!-- bullt points -->
						<br>Email :
						<a href="mailto:tangri.a@northeastern.edu">tangri.a@northeastern.edu</a>
						</p>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<!-- replace with my docs -->
						<li><a href="https://github.com/ArshT" target="_blank" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/arsh-tangri-a71194195/" target="_blank" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://scholar.google.com/citations?user=mYwF_I4AAAAJ&hl=en" target="_blank" class="icon fa-graduation-cap"><span class="label">Google Scholar</span></a></li>
						<li><a href="images/Arsh_Resume-1.pdf" class="icon solid fa-file" target="_blank"><span class="label">Resume</span></a></li>
					</ul>
					<ul class="copyright">
						<!-- <li>&copy; Untitled</li> -->
						<li>&copy; <a href="http://html5up.net">HTML5 UP</a></li>
					<!-- </ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>